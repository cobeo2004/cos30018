{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prediction Program\n",
    "### Predicting stock prices using Long Short-Term Memory (LSTM) Recurrent Neural Network using TensorFlow and Keras\n",
    "### Author: Abdeladim Fadheil\n",
    "### Adopter & Type Annotator: Simon Nguyen\n",
    "### Updated: Apr 2024\n",
    "### Version: P1\n",
    "### Tested environment: Python 3.11\n",
    "### Sees: https://thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Ready For Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading essential libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.17.0)\n",
      "Requirement already satisfied: pandas in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (3.9.2)\n",
      "Requirement already satisfied: yahoo_fin in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.8.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: lxml_html_clean in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: requests-html in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from yahoo_fin->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: feedparser in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from yahoo_fin->-r requirements.txt (line 5)) (6.0.11)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: lxml in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from lxml_html_clean->-r requirements.txt (line 7)) (5.2.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 1)) (0.44.0)\n",
      "Requirement already satisfied: rich in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: sgmllib3k in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from feedparser->yahoo_fin->-r requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: pyquery in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests-html->yahoo_fin->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests-html->yahoo_fin->-r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: parse in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests-html->yahoo_fin->-r requirements.txt (line 5)) (1.20.2)\n",
      "Requirement already satisfied: bs4 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests-html->yahoo_fin->-r requirements.txt (line 5)) (0.0.2)\n",
      "Requirement already satisfied: w3lib in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests-html->yahoo_fin->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from requests-html->yahoo_fin->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin->-r requirements.txt (line 5)) (8.2.0)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin->-r requirements.txt (line 5)) (11.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin->-r requirements.txt (line 5)) (4.66.5)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin->-r requirements.txt (line 5)) (10.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from bs4->requests-html->yahoo_fin->-r requirements.txt (line 5)) (4.12.3)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from pyquery->requests-html->yahoo_fin->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin->-r requirements.txt (line 5)) (3.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages (from beautifulsoup4->bs4->requests-html->yahoo_fin->-r requirements.txt (line 5)) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing essential libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Yahoo Finance\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "# Data Manipulation and Typing\n",
    "from collections import deque\n",
    "from numpy.typing import ArrayLike\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from typing_extensions import Annotated, Doc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Data Visualization\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up seeds, in order to receive a stable results after several training sessions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 314\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare the neccessary datasets to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffling two arrays in the same way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleInUnison(a: Annotated[ArrayLike, Doc(\"the first array to shuffle\")], b: Annotated[ArrayLike, Doc(\"the second array to shuffle\")]) -> None:\n",
    "    assert len(a) == len(b)\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `loadData()` function for loading data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.**\n",
    "\n",
    "This function is long but handy, and it accepts several arguments to be as flexible as possible:\n",
    "- The `ticker` argument is the ticker we want to load. For instance, you can use TSLA for the Tesla stock market, AAPL for Apple, and so on. It can also be a pandas Dataframe with the condition it includes the columns in feature_columns and date as an index.\n",
    "- `n_steps` integer indicates the historical sequence length we want to use; some people call it the window size, recall that we are going to use a recurrent neural network, we need to feed into the network a sequence data, choosing 50 means that we will use 50 days of stock prices to predict the next lookup time step.\n",
    "- `scale` is a boolean variable that indicates whether to scale prices from 0 to 1; we will set this to True as scaling high values from 0 to 1 will help the neural network to learn much faster and more effectively.\n",
    "- `lookup_step` is the future lookup step to predict, the default is set to 1 (e.g., next day). 15 means the next 15 days, and so on.\n",
    "- `split_by_date` is a boolean that indicates whether we split our training and testing sets by date. Setting it to False means we randomly split the data into training and testing using sklearn's train_test_split() function. If it's True (the default), we split the data in date order.\n",
    "\n",
    "We will use all the features available in this dataset: open, high, low, volume, and adjusted close. Please check this tutorial to learn more about what these indicators are.\n",
    "\n",
    "The below function does the following:\n",
    "\n",
    "- First, it loads the dataset using stock_info.get_data() function in yahoo_fin module.\n",
    "- It adds the \"date\" column from the index if it doesn't exist, this will help us later to get the features of the testing set.\n",
    "- If the scale argument is passed as True, it will scale all the prices from 0 to 1 (including the volume) using sklearn's MinMaxScaler class. Note that each column has its own scaler.\n",
    "- It then adds the future column, which indicates the target values (the labels to predict, or the y's) by shifting the adjusted close column by lookup_step.\n",
    "- After that, it shuffles and splits the data into training and testing sets and finally returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(ticker: Annotated[str | pd.DataFrame, Doc(\"the ticker you want to load, examples include AAPL, TESL, etc.\")],\n",
    "             n_steps: Annotated[int, Doc(\"the historical sequence length (i.e window size) used to predict, default is 50\")] = 50,\n",
    "             scale: Annotated[bool, Doc(\"whether to scale prices from 0 to 1, default is True\")] = True,\n",
    "             shuffle: Annotated[bool, Doc(\"whether to shuffle the dataset (both training & testing), default is True\")] = True,\n",
    "             lookup_step: Annotated[int, Doc(\"the future lookup step to predict, default is 1 (e.g next day)\")] = 1,\n",
    "             split_by_date: Annotated[bool, Doc(\"whether we split the dataset into training/testing by date, setting it to False will split datasets in a random way\")] = True,\n",
    "             test_size: Annotated[float, Doc(\"ratio for test data, default is 0.2 (20% testing data)\")] = 0.2,\n",
    "             feature_columns: Annotated[List[str], Doc(\"the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\")] = ['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # Check if ticker is already a loaded stock from yahoo_fin\n",
    "    if isinstance(ticker, str):\n",
    "        # If a string -> load data from Yahoo Finance\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # If a DataFrame -> use it\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"Invalid input type for ticker. Please provide a string or a pandas DataFrame.\")\n",
    "\n",
    "    # A dictionary to store the returned values from this function\n",
    "    result: Dict[str, Any] = {}\n",
    "    # Add a copy dataframe to the result\n",
    "    result['df'] = df.copy()\n",
    "    # Ensure that the feature_columns are in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"Error: '{col}' not found in the DataFrame columns.\"\n",
    "    # Add date as a column to the dataframe\n",
    "    if \"date\" not in df.columns:\n",
    "        df['date'] = df.index\n",
    "    if scale:\n",
    "        column_scaler: Dict[str, Any] = {}\n",
    "        # Scale the data (prices) from 0 to 1\n",
    "        for col in feature_columns:\n",
    "            scaler: MinMaxScaler = preprocessing.MinMaxScaler()\n",
    "            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "            column_scaler[col] = scaler\n",
    "        # Add the MinMaxScaler to the result\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # Add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # Since `lookup_step` columns contains NaN in future column -> get em beffore dropping NaNs\n",
    "    last_sequence: NDArray | list[NDArray] = np.array(df[feature_columns].tail(n_steps))\n",
    "    # Drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data: List[List[NDArray]] = []\n",
    "    sequences: deque = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # Get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # For example, suppose n_step = 50 and lookup_step = 1, then the last sequence should be 60 (50 + 10) length\n",
    "    # This last sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    # Add the last sequence to the result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # Construct the X's and Y's\n",
    "    X: List | NDArray = []\n",
    "    Y: List | NDArray = []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        Y.append(target)\n",
    "    # Convert the lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if split_by_date:\n",
    "        # If split_by_date is True, split the dataset into training & testing by date (not randomly)\n",
    "        train_samples: int = int((1 - test_size) * len(X))\n",
    "        result['X_train'] = X[:train_samples]\n",
    "        result['X_test'] = X[train_samples:]\n",
    "        result['Y_train'] = Y[train_samples:]\n",
    "        result['Y_test'] = Y[train_samples:]\n",
    "        if shuffle:\n",
    "            shuffleInUnison(result['X_train'], result['Y_train'])\n",
    "            shuffleInUnison(result['X_test'], result['Y_test'])\n",
    "    else:\n",
    "        # If split_by_date is False, split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"Y_train\"], result[\"Y_test\"] = train_test_split(X, Y, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # Get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # Retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # Remove duplicated dates in the tesiting dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # Remove dates from the training / testing sets and convert to float32 arr\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the `createModel()` function to build the model**\n",
    "Again, this function is flexible too, and you can change the number of layers, dropout rate, the RNN cell, loss, and the optimizer used to compile the model.\n",
    "\n",
    "The below function constructs an RNN with a dense layer as an output layer with one neuron. This model requires a sequence of features of `sequence_length` (in this case, we will pass 50 or 100) consecutive time steps (which are days in this dataset) and outputs a single value which indicates the price of the next time step.\n",
    "\n",
    "It also accepts `n_features` as an argument, which is the number of features we will pass on each sequence, in our case, we'll pass adjclose, [`open, high, low and volume`] columns (i.e 5 features).\n",
    "\n",
    "You can tweak the default parameters as you wish, `n_layers` is the number of RNN layers you want to stack, dropout is the dropout rate after each RNN layer, units are the number of RNN cell units (whether it is LSTM, SimpleRNN, or GRU), bidirectional is a boolean that indicates whether to use bidirectional RNNs, experiment with those!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(sequence_length: Annotated[int, Doc(\"the historical sequence length (i.e window size) used to predict, default is 50\")] = 50,\n",
    "                n_features: Annotated[int, Doc(\"the number of features to use to feed into the model, default is 5\")] = 5,\n",
    "                units: Annotated[int, Doc(\"the number of RNN cell units, default is 50\")] = 50,\n",
    "                n_layers: Annotated[int, Doc(\"the number of RNN layers, default is 1\")] = 1,\n",
    "                cell: Annotated[tf.keras.layers.Layer, Doc(\"the RNN cell to use, default is LSTM\")] = LSTM,\n",
    "                dropout: Annotated[float, Doc(\"the dropout rate, default is 0.2\")] = 0.2,\n",
    "                optimizer: Annotated[str, Doc(\"the optimizer to use, default is rmsprop\")] = \"rmsprop\",\n",
    "                loss: Annotated[str, Doc(\"the loss function to use, default is mean_absolute_error\")] = \"mean_absolute_error\",\n",
    "                bidirectional: Annotated[bool, Doc(\"whether to use bidirectional RNNs, default is False\")] = False) -> Sequential:\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With all the core functions ready, let's start by training the model, however, we need to initialize the parameters first so that we could customize it later on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is all about defining all the hyperparameters we gonna use; we explained some of them while we didn't explain the others:\n",
    "\n",
    "- `TEST_SIZE`: The testing set rate. For instance, 0.2 means 20% of the total dataset.\n",
    "- `FEATURE_COLUMNS`: The features we gonna use to predict the next price value.\n",
    "- `N_LAYERS`: Number of RNN layers to use.\n",
    "- `CELL`: RNN cell to use, default is LSTM.\n",
    "- `UNITS`: Number of cell units.\n",
    "- `DROPOUT`: The dropout rate is the probability of not training a given node in a layer, where 0.0 means no dropout at all. This regularization can help the model not overfit our training data. Check this tutorial for more information about dropout regularization.\n",
    "- `BIDIRECTIONAL`: Whether to use bidirectional recurrent neural networks.\n",
    "- `LOSS`: Loss function to use for this regression problem, we're using Huber loss, you can use mean absolute error (mae) or mean squared error (mse) as well.\n",
    "- `OPTIMIZER`: Optimization algorithm to use, defaulting to Adam.\n",
    "- `BATCH_SIZE`: The number of data samples to use on each training iteration.\n",
    "- `EPOCHS`: The number of times the learning algorithm will pass through the entire training dataset, we used 500 here, but try to increase it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we make sure that the `logs`, `results` and `data` folder must exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"logs\"):\n",
    "    os.makedirs(\"logs\")\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, let's call the `loadData()` and `createModel()` function to train the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cobeo/miniconda3/envs/cos30019-env-w1-p1/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0066 - mean_absolute_error: 0.0502\n",
      "Epoch 1: val_loss improved from inf to 0.00050, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 162ms/step - loss: 0.0066 - mean_absolute_error: 0.0500 - val_loss: 4.9782e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 2/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 7.8091e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 2: val_loss improved from 0.00050 to 0.00047, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - loss: 7.8075e-04 - mean_absolute_error: 0.0196 - val_loss: 4.6796e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 3/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 6.9940e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 3: val_loss improved from 0.00047 to 0.00043, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - loss: 6.9916e-04 - mean_absolute_error: 0.0183 - val_loss: 4.3293e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 4/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 7.4192e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 4: val_loss did not improve from 0.00043\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - loss: 7.4222e-04 - mean_absolute_error: 0.0195 - val_loss: 5.9813e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 5/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 6.8029e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 5: val_loss did not improve from 0.00043\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 192ms/step - loss: 6.8058e-04 - mean_absolute_error: 0.0192 - val_loss: 9.6708e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 6/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 8.4421e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 6: val_loss did not improve from 0.00043\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 196ms/step - loss: 8.4276e-04 - mean_absolute_error: 0.0213 - val_loss: 6.8323e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 7/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 7.0262e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 7: val_loss did not improve from 0.00043\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - loss: 7.0269e-04 - mean_absolute_error: 0.0189 - val_loss: 5.1419e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 8/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 7.0577e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 8: val_loss improved from 0.00043 to 0.00042, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 196ms/step - loss: 7.0531e-04 - mean_absolute_error: 0.0190 - val_loss: 4.1701e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 9/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 6.7395e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 9: val_loss did not improve from 0.00042\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - loss: 6.7367e-04 - mean_absolute_error: 0.0182 - val_loss: 6.6517e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 10/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 7.8696e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 10: val_loss did not improve from 0.00042\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - loss: 7.8580e-04 - mean_absolute_error: 0.0202 - val_loss: 5.5061e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 11/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 6.9832e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 11: val_loss did not improve from 0.00042\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 6.9826e-04 - mean_absolute_error: 0.0190 - val_loss: 5.5547e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 12/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 6.8044e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 12: val_loss improved from 0.00042 to 0.00041, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 6.7978e-04 - mean_absolute_error: 0.0188 - val_loss: 4.1224e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 13/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 5.9803e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 13: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 5.9843e-04 - mean_absolute_error: 0.0176 - val_loss: 5.0968e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 14/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 6.8446e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 14: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - loss: 6.8356e-04 - mean_absolute_error: 0.0186 - val_loss: 4.4015e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 15/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 7.2904e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 15: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - loss: 7.2858e-04 - mean_absolute_error: 0.0195 - val_loss: 4.3391e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 16/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 6.5408e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 16: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 209ms/step - loss: 6.5366e-04 - mean_absolute_error: 0.0186 - val_loss: 5.7121e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 17/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 6.5020e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 17: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 198ms/step - loss: 6.4982e-04 - mean_absolute_error: 0.0188 - val_loss: 4.7338e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 18/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 5.9458e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 18: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 214ms/step - loss: 5.9439e-04 - mean_absolute_error: 0.0177 - val_loss: 4.3168e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 19/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 6.8176e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 19: val_loss improved from 0.00041 to 0.00041, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 6.8119e-04 - mean_absolute_error: 0.0188 - val_loss: 4.0866e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 20/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 5.5133e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 20: val_loss improved from 0.00041 to 0.00041, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 201ms/step - loss: 5.5110e-04 - mean_absolute_error: 0.0175 - val_loss: 4.0593e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 21/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 5.5648e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 21: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 214ms/step - loss: 5.5638e-04 - mean_absolute_error: 0.0180 - val_loss: 4.1407e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 22/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 6.1953e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 22: val_loss did not improve from 0.00041\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 193ms/step - loss: 6.1915e-04 - mean_absolute_error: 0.0187 - val_loss: 4.1314e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 23/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 5.8892e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 23: val_loss improved from 0.00041 to 0.00040, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - loss: 5.8837e-04 - mean_absolute_error: 0.0180 - val_loss: 4.0021e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 24/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 5.2818e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 24: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 210ms/step - loss: 5.2841e-04 - mean_absolute_error: 0.0174 - val_loss: 4.8411e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 25/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 5.7964e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 25: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 5.7934e-04 - mean_absolute_error: 0.0179 - val_loss: 4.1377e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 26/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 5.5627e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 26: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - loss: 5.5617e-04 - mean_absolute_error: 0.0182 - val_loss: 4.2849e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 27/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 5.4863e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 27: val_loss improved from 0.00040 to 0.00040, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 198ms/step - loss: 5.4843e-04 - mean_absolute_error: 0.0176 - val_loss: 3.9951e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 28/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 5.5363e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 28: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - loss: 5.5360e-04 - mean_absolute_error: 0.0181 - val_loss: 4.7159e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 29/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 6.2607e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 29: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - loss: 6.2565e-04 - mean_absolute_error: 0.0189 - val_loss: 4.0513e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 30/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 5.6306e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 30: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - loss: 5.6290e-04 - mean_absolute_error: 0.0185 - val_loss: 4.1908e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 31/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 5.7891e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 31: val_loss improved from 0.00040 to 0.00040, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 5.7858e-04 - mean_absolute_error: 0.0184 - val_loss: 3.9860e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 32/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 5.9922e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 32: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 211ms/step - loss: 5.9869e-04 - mean_absolute_error: 0.0187 - val_loss: 4.0925e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 33/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 5.4381e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 33: val_loss improved from 0.00040 to 0.00040, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - loss: 5.4380e-04 - mean_absolute_error: 0.0179 - val_loss: 3.9591e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 34/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 5.2670e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 34: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 196ms/step - loss: 5.2689e-04 - mean_absolute_error: 0.0180 - val_loss: 4.0289e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 35/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 5.3796e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 35: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 210ms/step - loss: 5.3817e-04 - mean_absolute_error: 0.0182 - val_loss: 5.0715e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 36/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 5.7544e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 36: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 214ms/step - loss: 5.7498e-04 - mean_absolute_error: 0.0186 - val_loss: 4.8732e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 37/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 5.3976e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 37: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 212ms/step - loss: 5.3976e-04 - mean_absolute_error: 0.0187 - val_loss: 4.0442e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 38/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 5.6112e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 38: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - loss: 5.6082e-04 - mean_absolute_error: 0.0184 - val_loss: 3.9940e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 39/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 4.7562e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 39: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - loss: 4.7575e-04 - mean_absolute_error: 0.0173 - val_loss: 4.2551e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 40/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 5.3460e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 40: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - loss: 5.3476e-04 - mean_absolute_error: 0.0186 - val_loss: 5.4744e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 41/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 6.0846e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 41: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 213ms/step - loss: 6.0856e-04 - mean_absolute_error: 0.0195 - val_loss: 5.3842e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 42/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 4.9764e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 42: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 210ms/step - loss: 4.9781e-04 - mean_absolute_error: 0.0180 - val_loss: 4.4033e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 43/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 5.2170e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 43: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - loss: 5.2168e-04 - mean_absolute_error: 0.0183 - val_loss: 4.5336e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 44/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 5.3451e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 44: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - loss: 5.3440e-04 - mean_absolute_error: 0.0182 - val_loss: 4.0582e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 45/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 5.5447e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 45: val_loss improved from 0.00040 to 0.00040, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - loss: 5.5435e-04 - mean_absolute_error: 0.0189 - val_loss: 3.9512e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 46/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 5.2186e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 46: val_loss did not improve from 0.00040\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - loss: 5.2182e-04 - mean_absolute_error: 0.0185 - val_loss: 4.1249e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 47/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 5.3572e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 47: val_loss improved from 0.00040 to 0.00039, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 209ms/step - loss: 5.3551e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9375e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 48/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 5.2936e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 48: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - loss: 5.2934e-04 - mean_absolute_error: 0.0182 - val_loss: 4.5379e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 49/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 4.7409e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 49: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 208ms/step - loss: 4.7418e-04 - mean_absolute_error: 0.0175 - val_loss: 3.9925e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 50/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 4.6026e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 50: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - loss: 4.6035e-04 - mean_absolute_error: 0.0177 - val_loss: 4.0069e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 51/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 5.6361e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 51: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 5.6350e-04 - mean_absolute_error: 0.0190 - val_loss: 4.0472e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 52/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 5.2370e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 52: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 5.2366e-04 - mean_absolute_error: 0.0188 - val_loss: 4.0116e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 53/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 4.9977e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 53: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 207ms/step - loss: 5.0002e-04 - mean_absolute_error: 0.0180 - val_loss: 4.5551e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 54/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 5.2147e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 54: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 5.2148e-04 - mean_absolute_error: 0.0186 - val_loss: 4.2043e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 55/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 5.1591e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 55: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 5.1583e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9612e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 56/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 5.1272e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 56: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - loss: 5.1279e-04 - mean_absolute_error: 0.0185 - val_loss: 5.0382e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 57/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 5.4940e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 57: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 5.4911e-04 - mean_absolute_error: 0.0194 - val_loss: 4.0958e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 58/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 4.8838e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 58: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - loss: 4.8835e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0538e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 59/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 4.6755e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 59: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - loss: 4.6741e-04 - mean_absolute_error: 0.0178 - val_loss: 3.9499e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 60/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 5.1475e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 60: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 190ms/step - loss: 5.1449e-04 - mean_absolute_error: 0.0186 - val_loss: 4.0303e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 61/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 5.3924e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 61: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - loss: 5.3915e-04 - mean_absolute_error: 0.0188 - val_loss: 4.2691e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 62/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 5.1278e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 62: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 193ms/step - loss: 5.1268e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0788e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 63/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 4.8984e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 63: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 4.8986e-04 - mean_absolute_error: 0.0182 - val_loss: 3.9560e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 64/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 5.5619e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 64: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 206ms/step - loss: 5.5615e-04 - mean_absolute_error: 0.0191 - val_loss: 4.1994e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 65/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 4.9039e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 65: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 201ms/step - loss: 4.9036e-04 - mean_absolute_error: 0.0186 - val_loss: 4.5981e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 66/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 5.0753e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 66: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 198ms/step - loss: 5.0738e-04 - mean_absolute_error: 0.0185 - val_loss: 4.0876e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 67/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 4.9791e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 67: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - loss: 4.9786e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9553e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 68/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 5.3442e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 68: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - loss: 5.3420e-04 - mean_absolute_error: 0.0188 - val_loss: 4.0508e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 69/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 5.3974e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 69: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 5.3964e-04 - mean_absolute_error: 0.0192 - val_loss: 4.1948e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 70/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 4.8226e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 70: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 196ms/step - loss: 4.8236e-04 - mean_absolute_error: 0.0183 - val_loss: 4.1445e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 71/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 5.4088e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 71: val_loss improved from 0.00039 to 0.00039, saving model to results/2024-08-16_AMZN-sh-1-sc-1-sbd-0-huber-adam-LSTM-seq-50-step-15-layers-2-units-256.weights.h5\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - loss: 5.4062e-04 - mean_absolute_error: 0.0193 - val_loss: 3.9213e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 72/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 4.9467e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 72: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 4.9476e-04 - mean_absolute_error: 0.0183 - val_loss: 4.4363e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 73/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 5.1436e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 73: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 201ms/step - loss: 5.1434e-04 - mean_absolute_error: 0.0186 - val_loss: 4.0646e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 74/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 5.2249e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 74: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - loss: 5.2226e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9737e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 75/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 4.8273e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 75: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 4.8273e-04 - mean_absolute_error: 0.0183 - val_loss: 4.1466e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 76/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 5.4038e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 76: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - loss: 5.4005e-04 - mean_absolute_error: 0.0192 - val_loss: 4.0738e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 77/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 5.5311e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 77: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - loss: 5.5268e-04 - mean_absolute_error: 0.0189 - val_loss: 4.2728e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 78/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 5.4635e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 78: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - loss: 5.4606e-04 - mean_absolute_error: 0.0192 - val_loss: 3.9714e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 79/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 4.9395e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 79: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - loss: 4.9378e-04 - mean_absolute_error: 0.0185 - val_loss: 4.0877e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 80/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 4.7077e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 80: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - loss: 4.7081e-04 - mean_absolute_error: 0.0179 - val_loss: 3.9632e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 81/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 4.8852e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 81: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - loss: 4.8874e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0344e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 82/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 4.9244e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 82: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - loss: 4.9248e-04 - mean_absolute_error: 0.0183 - val_loss: 4.3951e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 83/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 5.2461e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 83: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 212ms/step - loss: 5.2437e-04 - mean_absolute_error: 0.0193 - val_loss: 4.1596e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 84/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 5.0016e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 84: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 206ms/step - loss: 5.0000e-04 - mean_absolute_error: 0.0185 - val_loss: 4.2056e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 85/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 5.3544e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 85: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - loss: 5.3541e-04 - mean_absolute_error: 0.0189 - val_loss: 3.9686e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 86/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 5.0091e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 86: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 207ms/step - loss: 5.0070e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9987e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 87/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 4.6084e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 87: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 4.6090e-04 - mean_absolute_error: 0.0179 - val_loss: 3.9830e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 88/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 4.9187e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 88: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 210ms/step - loss: 4.9198e-04 - mean_absolute_error: 0.0182 - val_loss: 4.2975e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 89/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 5.0761e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 89: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - loss: 5.0804e-04 - mean_absolute_error: 0.0185 - val_loss: 4.0487e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 90/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 5.4390e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 90: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - loss: 5.4360e-04 - mean_absolute_error: 0.0195 - val_loss: 4.0667e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 91/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 5.1272e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 91: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 198ms/step - loss: 5.1247e-04 - mean_absolute_error: 0.0186 - val_loss: 4.1552e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 92/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 4.7578e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 92: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205ms/step - loss: 4.7554e-04 - mean_absolute_error: 0.0182 - val_loss: 3.9482e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 93/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 4.7014e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 93: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - loss: 4.7028e-04 - mean_absolute_error: 0.0179 - val_loss: 4.0343e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 94/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 4.6753e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 94: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 4.6760e-04 - mean_absolute_error: 0.0179 - val_loss: 4.6601e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 95/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 4.9402e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 95: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 193ms/step - loss: 4.9382e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9936e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 96/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 5.4318e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 96: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - loss: 5.4295e-04 - mean_absolute_error: 0.0192 - val_loss: 4.0129e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 97/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 5.3339e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 97: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - loss: 5.3319e-04 - mean_absolute_error: 0.0189 - val_loss: 4.0723e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 98/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 4.8186e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 98: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - loss: 4.8202e-04 - mean_absolute_error: 0.0184 - val_loss: 3.9739e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 99/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 4.9601e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 99: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - loss: 4.9598e-04 - mean_absolute_error: 0.0185 - val_loss: 3.9809e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 100/100\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 4.8834e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 100: val_loss did not improve from 0.00039\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - loss: 4.8810e-04 - mean_absolute_error: 0.0180 - val_loss: 4.0817e-04 - val_mean_absolute_error: 0.0132\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = loadData(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "data['df'].to_csv(ticker_data_filename)\n",
    "# Create the model\n",
    "model = createModel(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, optimizer=OPTIMIZER, cell=CELL, dropout=DROPOUT, n_layers=N_LAYERS, units=UNITS, bidirectional=BIDIRECTIONAL)\n",
    "# Some callback function for tensorboard\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".weights.h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# Train the model and save the weights whenever we see a new optimal model using ModalCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"Y_train\"], batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(data[\"X_test\"], data[\"Y_test\"]), callbacks=[checkpointer, tensorboard], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After the training ends (or during the training), try to run tensorboard using this command. The tensorboard will be run at `localhost:6006`**\n",
    "\n",
    "The loss is Huber loss as specified in the LOSS parameter (you can always change it to mean absolute error or mean squared error), the curve is the validation loss. As you can see, it is significantly decreasing over time. You can also increase the number of epochs to get much better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes a pandas Dataframe and plots the true and predicted prices in the same plot using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(test_df: Annotated[pd.DataFrame, Doc(\"the testing dataframe\")]):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f\"true_adjclose_{LOOKUP_STEP}\"], c='b')\n",
    "    plt.plot(test_df[f\"adjclose_{LOOKUP_STEP}\"], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes the model and the data that was returned by `createModel()` and `loadData()` functions respectively, and constructs a dataframe that includes the predicted adjclose along with true future adjclose, as well as calculating buy and sell profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalDF(model: Annotated[Sequential, Doc(\"the model to use\")], data: Annotated[Dict[str, Any], Doc(\"the data to use\")]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes the model and the data that was returned by `createModel()` and `loadData()` functions respectively, and constructs a dataframe that includes the predicted adjclose along with true future adjclose, as well as calculating buy and sell profit.\n",
    "    \"\"\"\n",
    "    # if predicted future price > current -> calculate the true future price - current price, to get the buy profit\n",
    "    buy_profit = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if predicted future price < current -> calculate the current price - true future price, to get the sell profit\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    # Perform prediction and get prices\n",
    "    X_test = data[\"X_test\"]\n",
    "    Y_test = data[\"Y_test\"]\n",
    "    Y_Predicted = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        Y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(Y_test, axis=0)))\n",
    "        Y_Predicted = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(Y_Predicted))\n",
    "\n",
    "    test_df = data[\"test_df\"]\n",
    "    # Add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = Y_Predicted\n",
    "    # Add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = Y_test\n",
    "    # Sort dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # Add the buy profit columns\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, final_df[\"adjclose\"], final_df[f\"adjclose_{LOOKUP_STEP}\"], final_df[f\"true_adjclose_{LOOKUP_STEP}\"]))\n",
    "    # Add the sell profit columns\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, final_df[\"adjclose\"], final_df[f\"adjclose_{LOOKUP_STEP}\"], final_df[f\"true_adjclose_{LOOKUP_STEP}\"]))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last function we going to define is responsible for predicting the next future price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: Annotated[Sequential, Doc(\"the model to use\")], data: Annotated[Dict[str, Any], Doc(\"the data to use\")]):\n",
    "    \"\"\"\n",
    "    This function takes the model and the data that was returned by `createModel()` and `loadData()` functions respectively, and predicts the next future price.\n",
    "    \"\"\"\n",
    "    # Retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # Expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # Get the prediction (scaled from 0 -> 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # Get the price value by inverting the scaling\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load optimal weights and proceed with evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"results\", model_name) + \".weights.h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `loss` and `mean absolute error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.584657595144968\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"Y_test\"], verbose=0)\n",
    "\n",
    "# Inverse scaling to calculate the the mean absolute error\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the final dataframe by using `getFinalDF()` for construct the testing set dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n"
     ]
    }
   ],
   "source": [
    "final_df = getFinalDF(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `predict()` to get the future price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score by counting the number of positive profits (both buy and sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 169.81$\n",
      "huber loss: 0.00039213348645716906\n",
      "Mean Absolute Error: 2.584657595144968\n",
      "Accuracy score: 0.5459896983075792\n",
      "Total buy profit: 650.449882209301\n",
      "Total sell profit: 99.70385867357251\n",
      "Total profit: 750.1537408828735\n",
      "Profit per trade: 0.5519895076400836\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df[\"sell_profit\"] > 0]) + len(final_df[final_df[\"buy_profit\"] > 0])) / len(final_df)\n",
    "\n",
    "# Total buy and sell profit\n",
    "total_buy_profit = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "\n",
    "# Total profit of both buy and sell\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "\n",
    "# Number of profit per trades\n",
    "profit_per_trade = total_profit / len(final_df)\n",
    "\n",
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos30019-env-w1-p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

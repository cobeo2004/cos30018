{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction using LSTM\n",
    "# \n",
    "## This notebook demonstrates how to predict stock prices using a Long Short-Term Memory (LSTM) neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This week's (Week 3) tasks:\n",
    "### Write a function to load and process a dataset with multiple features with the following requirements:\n",
    "1. Write a function to display stock market financial data using candlestick chart. You can use the following tutorial: https://coderzcolumn.com/tutorials/data-science/candlestick-chart-in-python-mplfinance-plotly-bokeh but again, you will need to explain in details all the code in your program (including the meanings of the arguments in a function call). Furthermore, you need to include an option in your function to allow each candle stick to express the data of n trading days (n ≥ 1).\n",
    "2. Write a function to display stock market financial data using boxplot chart. This is particularly useful when you are trying to display your data for a moving window of n consecutive trading days. Again, feel free to use online resources that teach you how to do this but you need to add comments to explain your codes and the parameters you use.\n",
    "3. Upload your Task 3 Report (as a PDF file) to the project Wiki before the deadline and email your project leader to notify that it is ready for viewing and feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Virtual Environment using `Conda` (If you have your own `Virutal Environment` your then ignore this !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To activate the conda environment, copy the below content to `conda-config.sh` and run the following command in the terminal:\n",
    "# sh conda-config.sh\n",
    "\n",
    "'''conda-config.sh\n",
    "\n",
    "conda create -n cos30019_env_w3_v0.2 python=3.10.9\n",
    "conda activate cos30019_env_w3_v0.2\n",
    "#Check current environment\n",
    "conda info --envs\n",
    "#Check current python version\n",
    "python --version\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas-datareader\n",
    "# !pip install yfinance\n",
    "# !pip install -q talib\n",
    "# !pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import pandas_datareader as pdr\n",
    "# import datetime as dt\n",
    "import yfinance as yf\n",
    "import talib as ta\n",
    "import mplfinance as mpf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, InputLayer, Input, Activation\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from typing_extensions import Annotated, Doc, TypeVar, Literal, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare essential variables\n",
    "- Define the start date, the end date and the ticker (stock) that we wanted to download from `yfinance`.\n",
    "- Define the split ratio for splitting the train and test data, and the number of days for looking back.\n",
    "- Define the directory for storing the raw and prepared datasets.\n",
    "- Define the files that are used for storing essential information, such as raw data imported from yfinance and the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start, end and ticker for the datasets\n",
    "start=\"2015-01-01\"\n",
    "end=\"2023-08-25\"\n",
    "ticker=\"TSLA\"\n",
    "\n",
    "# Split ratio, 0.8 equals to 80% data for training and 20% for testing\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Number of days to look back for the prediction, could be changed to any value\n",
    "num_look_back_days = 30\n",
    "\n",
    "# Define entry directory for the raw and prepared datasets\n",
    "ENTRY_POINT = f\"datasets/{ticker}/from_{start}_to_{end}\"\n",
    "RAW_DATA_DIRECTORY = os.path.join(ENTRY_POINT, \"data\")\n",
    "PREPARED_DATA_DIRECTORY = os.path.join(ENTRY_POINT, \"prepared-data\")\n",
    "\n",
    "# Raw data to be saved as the same for\n",
    "RAW_CSV_FILE = os.path.join(RAW_DATA_DIRECTORY, f\"raw_data_from_{start}_to_{end}_of_{ticker}_stock.csv\")\n",
    "\n",
    "# Prepared data to be saved as the same for\n",
    "PREPARED_CSV = os.path.join(PREPARED_DATA_DIRECTORY, f\"prepared_data_from_{start}_to_{end}_of_{ticker}_stock.csv\")\n",
    "PREPARED_TRAIN_ALL = os.path.join(PREPARED_DATA_DIRECTORY, f\"xytrain_data_{start}-{end}-{ticker}_stock.npz\")\n",
    "PREPARED_TRAIN_DATASET = os.path.join(PREPARED_DATA_DIRECTORY, f\"train_dataset_of_{ticker}_from_{start}_to_{end}.csv\")\n",
    "PREPARED_TEST_DATASET = os.path.join(PREPARED_DATA_DIRECTORY, f\"test_dataset_of_{ticker}_from_{start}_to_{end}.csv\")\n",
    "PREPARED_SCALER_FEATURE = os.path.join(PREPARED_DATA_DIRECTORY, f\"feature_scaler_of_{ticker}_from_{start}_to_{end}.pkl\")\n",
    "PREPARED_SCALER_TARGET = os.path.join(PREPARED_DATA_DIRECTORY, f\"targe_scaler_of_{ticker}_from_{start}_to_{end}.pkl\")\n",
    "PREPARED_TRAIN_ARRAY = os.path.join(PREPARED_DATA_DIRECTORY, f\"xytrain_train_array_of_{ticker}_from_{start}_to_{end}.npz\")\n",
    "PREPARED_TEST_ARRAY = os.path.join(PREPARED_DATA_DIRECTORY, f\"xytrain_test_array_of_{ticker}_from_{start}_to_{end}.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that directory must exists\n",
    "- check_directory_exists() & create_directory(): The purpose of these two functions are two check if the given dir_path is exists or not using os.path.isdir(), if not then the check_directory_exists() will return a False and use create_directory() to create a folder based on the given dir_path, otherwise return True. \n",
    "- check_file_exists(): This function will check if the file exists using os.path.exists() function, it will return True if exists otherwise return False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory_exists(dir_path: Annotated[str, Doc(\"The path to the directory to be checked\")]) -> bool:\n",
    "    return True if os.path.isdir(dir_path) else False\n",
    "\n",
    "def create_directory(dir_path: Annotated[str, Doc(\"The path to the directory to be created\")]) -> None:\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "def check_file_exists(file_path: Annotated[str, Doc(\"The path to the file to be checked\")]) -> bool:\n",
    "    return True if os.path.exists(file_path) else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to load and save data objects\n",
    "**The load_and_save_object() function does the following procedure:**\n",
    "- If the mode is set to save, the function will start opening up the file defined in fn parameter. Once the file is successfully opened, it will write all of the given objects defined in obj by using pickle.dump() to ensure that the object will be type-safe when loading the object from the file.\n",
    "- If the mode is set to load, the function will start reading from the file defined in fn and load the object using pickle.load() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\", bound=Literal[\"load\", \"save\"])\n",
    "U = TypeVar(\"U\", any, object)\n",
    "def save_or_load_object(obj: Annotated[U | None, Doc(\"The object to be saved or loaded\")],\n",
    "                         fn: Annotated[str, Doc(\"The filename to the directory to be checked\")],\n",
    "                         mode: Annotated[T, Doc(\"The mode to be used\")]) -> U | None:\n",
    "    # Save the object to the file if the mode is save\n",
    "    if mode == \"save\":\n",
    "        with open(fn, \"wb\") as i:\n",
    "            pickle.dump(obj, i)\n",
    "    # Load the object from the file if the mode is load\n",
    "    elif mode == \"load\":\n",
    "        # Check if the object is None, if so then load the object from the file\n",
    "        if obj is None:\n",
    "            with open(fn, \"rb\") as i:\n",
    "                return pickle.load(i)\n",
    "        else:\n",
    "            raise ValueError(\"obj must be None when mode is load\")\n",
    "    # Raise an error if the mode is invalid\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data function. Check if exists data, if not then load and save from the `yfinance`\n",
    "- **The major purpose of this function is to check if the unprocessed CSV file that contains the stock data from the start date and end date is existed in the file or not, if it is not existed then it will download the data from yfinance and save it to the directory that contains unprocessed CSV data. Otherwise, it will load the unprocessed CSV File. The following procedures are:**\n",
    "    - The code will check if the directory that contains unprocessed CSV file exists, if not then it will start to create the directory and download the CSV from the yfinance. Once it’s downloaded then it will convert the data to CSV Format and save it to the created directory.\n",
    "    - However, in the case where the directory exists then it will read the file in that directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(start: Annotated[str, Doc(\"The start date of the stock to be loaded\")],\n",
    "              end: Annotated[str, Doc(\"The end date of the stock to be loaded\")],\n",
    "              ticker: Annotated[str, Doc(\"The ticker symbol of the stock to be loaded\")] = \"CBA.AX\"):\n",
    "    # Check if the raw data directory exists, if not then create it\n",
    "    if not check_directory_exists(RAW_DATA_DIRECTORY):\n",
    "        create_directory(RAW_DATA_DIRECTORY)\n",
    "        data = yf.download(ticker, start=start, end=end)\n",
    "        data.to_csv(RAW_CSV_FILE)\n",
    "    else:\n",
    "        # Load the data from the local machine\n",
    "        data = pd.read_csv(RAW_CSV_FILE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by ratio\n",
    "**This function will do the following procedures:**\n",
    "- If is_split_by_date is set to True: This function will perform calculation based on the split ratio (0.8 will equal 80% of train data and 20% of test data). Once the calculation is finished, the function will allocate the train and test data to the train_data and test_data variables.\n",
    "- If is_split_by_date is set to False: This function will use the built-in function provided by scikit-learn, which is train_test_split() to split the data randomly based on the given ratio value. The random_state is fixed at 42 to make sure that the splitted data is reproducible.\n",
    "- Once the data has been successfully splitted, it will then print out the shape (x,y) of both train and test data and return it to the user for further usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_ratio(data: Annotated[pd.DataFrame, Doc(\"The data to be split\")],\n",
    "                        ratio: Annotated[float, Doc(\"The ratio to be used in percentage: 0.8 for 80% train and 20% test\")],\n",
    "                        is_split_by_date: Annotated[bool, Doc(\"Choose to split by date or random\")] = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Check if the data is split by date\n",
    "    if is_split_by_date:\n",
    "        # Calculate the train data size of the train data\n",
    "        train_size = int(len(data) * ratio)\n",
    "        # Split the data into train and test data\n",
    "        train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]\n",
    "    else:\n",
    "        # Split the data into train and test data randomly\n",
    "        train_data, test_data = train_test_split(data,train_size=1 - (1 - ratio), test_size= 1 - ratio, random_state=42)\n",
    "\n",
    "    # Print the shape of the train and test data\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data using `MinMaxScaler()`\n",
    "- **This function will check if the is_scale equals to True or not, if it is not then the function will simply return back the data without any scaling data. Otherwise, it will do the following procedures:**\n",
    "    - Scaling data: This function will use the MinMaxScaler class provided by scikit-learn to scale the data to the range of (0, 1) by setting the feature_range to a tuple of (0, 1).\n",
    "    - Reshaping data: After the data is scaled, the data will be converted into a two-dimensional array using reshape(-1, 1) function. This procedure is essential as the fit_transform() function only accepts a 2D array.\n",
    "    - Fitting and returning the scaled array: Once the array is transformed to 2D array, it will be fitted using the fit_transform() function provided by class MinMaxScaler. Once the transform is finished we will return the scaled data with the instance of MinMaxScaler for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(data: Annotated[pd.DataFrame, Doc(\"The data to be scaled\")], is_scale: Annotated[bool, Doc(\"Choose to scale or not, default set to True\")] = True) -> Tuple[pd.DataFrame, MinMaxScaler | None]:\n",
    "    if is_scale:\n",
    "        # Using MinMaxScaler to scale the data to the range of (0, 1)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        if len(data.shape) == 1:\n",
    "            # If the data array is 1D, convert to 2D array\n",
    "            data = data.values.reshape(-1, 1)\n",
    "        # Fit the 2D-transformed data into the scaler\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "        return scaled_data, scaler\n",
    "    else:\n",
    "        return data, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate data\n",
    "**This function will check if the directory that contains processed data exists, if not then it will create a directory used for storing processed data.\n",
    "Once the directory is created, it will check the existence of the processed CSV file. If the CSV file exists, it will read the CSV file using pd.read_csv() and return it. Otherwise, it will start the following procedure**:\n",
    "- Read the unprocessed CSV file: It will read the unprocessed CSV file stored in the unprocessed folder.\n",
    "- Convert and set index: After the unprocessed CSV file is successfully read, it will convert the value of Date column into the DateTime type and set the value inside the Date column as index. This will make it easier to operate time-based searching, simplify the plotting procedure and enhance the data organization.\n",
    "- Adding essential indicators to the data: This function will also add several required technical analysis indicators to the data, such as Relative Strength Index (RSI) and several different date ranges (20 days, 100 days and 150 days) of Exponential Moving Averages (EMA) using the built-in functions provided by TA-lib.\n",
    "- Calculate target price: After adding indicators to the data, it will start calculating the target price by subtracting the value of Adjustment Close Price and the Open Price. Once subtracted, it will shift the value back by one to assume that is the targeted price.\n",
    "- Indicate if the value is increased or not: It will calculate the TargetClass based on the target price value and indicate if the price is increased or not, if the price is increased then the TargetClass value will be 1, otherwise it will be 0.\n",
    "- Drop all undefined values: By using dropna() function, it will look up all of the undefined (NaN) values and drop it.\n",
    "- Save processed data: Once the datasets are processed, it will save those datasets into a CSV File by using to_csv() function and return the processed datasets to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data() -> pd.DataFrame:\n",
    "    # Check if the prepared data directory exists, if not then create it\n",
    "    if not check_directory_exists(PREPARED_DATA_DIRECTORY):\n",
    "        create_directory(PREPARED_DATA_DIRECTORY)\n",
    "\n",
    "    if check_file_exists(PREPARED_CSV):\n",
    "        print('Loading Prepared Data...')\n",
    "        df = pd.read_csv(PREPARED_CSV)\n",
    "    else:\n",
    "        print('Processing Raw Data...')\n",
    "        # Read the raw data\n",
    "        df = pd.read_csv(RAW_CSV_FILE)\n",
    "        # Convert the date column to datetime\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            # Set the date column as the index\n",
    "            df.set_index('Date', inplace=True)\n",
    "        print(\"Type of index after converted: \", type(df.index))\n",
    "        # Adding RSI, EMA20(EMA for 20 days), EMA100(EMA for 100 days), EMA200(EMA for 200 days)\n",
    "        df['Close_RSI'] = ta.RSI(df['Close'], timeperiod=14)\n",
    "        df['Close_EMA20'] = ta.EMA(df['Close'], timeperiod=20)\n",
    "        df['Close_EMA100'] = ta.EMA(df['Close'], timeperiod=100)\n",
    "        df['Close_EMA200'] = ta.EMA(df['Close'], timeperiod=200)\n",
    "\n",
    "        # Calculate the target value by subtracting the open price\n",
    "        # and the adjusted close price and shifting it by 1 day\n",
    "        df['Target'] = df['Adj Close'] - df['Open']\n",
    "        df['Target'] = df['Target'].shift(-1)\n",
    "\n",
    "        # Convert the target class to binary class if the target is greater than 0\n",
    "        df['TargetClass'] = np.where(df['Target'] > 0, 1, 0)\n",
    "        # Shift the adjusted close price by 1 day\n",
    "        df['TargetNextClose'] = df['Adj Close'].shift(-1)\n",
    "\n",
    "        # Drop the NaN values\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Convert to csv and save to the folder\n",
    "        df.to_csv(PREPARED_CSV, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets\n",
    "**The given procedure, which is the heart to execute successfully the program, does the following steps:**\n",
    "- Load and validate data: The procedure initially calls to the defined load_data() function, which will either load the existing raw data or download the data if it does not exist. Once the data is loaded successfully, it will call the validate_data() function to process and validate the data to be in the right format, which adds in extra technical analysis indicators and target values.\n",
    "- Check the existence of the prepared datasets: After the data is validated, it will check if the train and test datasets exist. If the datasets are existed, it will do the following steps:\n",
    "    - Load from the processed CSV file the existing datasets, including train and test datasets.\n",
    "    - Print out the shapes of both train and test datasets for validation.\n",
    "    - Load the saved scaler values of both feature and target.\n",
    "    - Retrieve the value from the x and y array of both train and test arrays.\n",
    "- In case where the datasets are not existed, the following logic will be executed:\n",
    "    - The validated data will be split into train and test data using the defined split_data_by_ratio() function. With the split rate defined at 0.8 (80% of train data, 20% of test data).\n",
    "    - Scale the training data for the features using the defined scaling_data() function and based on the defined feature_cols columns.\n",
    "    - Make training arrays by creating a sequence for each sample based on the defined variable num_look_back_days.\n",
    "    - Scale the testing data for the features and target using the defined scaling_data() function and based on the defined feature_cols and target_cols columns.\n",
    "    - Make test arrays by creating a sequence of each sample based on the defined variable num_look_back_days.\n",
    "    - Save the train and test data to a CSV file by using the to_csv()function from pandas and save the feature and target scalers using the defined save_or_load_object() function.\n",
    "    - Save the created train and test arrays to a .npz file using savez()function from numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(start, end, ticker)\n",
    "df = validate_data()\n",
    "\n",
    "if check_file_exists(PREPARED_TRAIN_DATASET) and check_file_exists(PREPARED_TEST_DATASET):\n",
    "    print('Loading Existed Train and Test Data...')\n",
    "    train_data = pd.read_csv(PREPARED_TRAIN_DATASET)\n",
    "    test_data = pd.read_csv(PREPARED_TEST_DATASET)\n",
    "\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "    train_feature_scale = save_or_load_object(None, PREPARED_SCALER_FEATURE, \"load\")\n",
    "    train_target_scale = save_or_load_object(None, PREPARED_SCALER_TARGET, \"load\")\n",
    "    train_arrays = np.load(PREPARED_TRAIN_ARRAY)\n",
    "    x_train = train_arrays['x_train']\n",
    "    y_train = train_arrays['y_train']\n",
    "\n",
    "    test_arrays = np.load(PREPARED_TEST_ARRAY)\n",
    "    x_test = test_arrays['x_test']\n",
    "    y_test = test_arrays['y_test']\n",
    "else:\n",
    "    print('Processing Train and Test Data...')\n",
    "    train_data, test_data = split_data_by_ratio(df, split_ratio)\n",
    "\n",
    "    feature_cols = ['Open', 'High', 'Low', 'Close_RSI', 'Close_EMA20', 'Close_EMA100', 'Close_EMA200']\n",
    "    target_cols = 'TargetNextClose'\n",
    "\n",
    "    scaled_train_data, train_feature_scale = scaling_data(train_data[feature_cols])\n",
    "    converted_2d_train_data = train_data[target_cols].values.reshape(-1, 1)\n",
    "    scaled_train_target, train_target_scale = scaling_data(converted_2d_train_data)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(num_look_back_days, len(scaled_train_data)):\n",
    "        x_train.append(scaled_train_data[i-num_look_back_days:i])\n",
    "        y_train.append(scaled_train_target[i])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "    scaled_test_data = train_feature_scale.transform(test_data[feature_cols])\n",
    "    converted_2d_test_data = test_data[target_cols].values.reshape(-1, 1)\n",
    "    scaled_test_target = train_target_scale.transform(converted_2d_test_data)\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    for i in range(num_look_back_days, len(scaled_test_data)):\n",
    "        x_test.append(scaled_test_data[i-num_look_back_days:i])\n",
    "        y_test.append(scaled_test_target[i])\n",
    "\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "    train_data.to_csv(PREPARED_TRAIN_DATASET, index=False)\n",
    "    test_data.to_csv(PREPARED_TEST_DATASET, index=False)\n",
    "\n",
    "    save_or_load_object(train_feature_scale, PREPARED_SCALER_FEATURE, \"save\")\n",
    "    save_or_load_object(train_target_scale, PREPARED_SCALER_TARGET, \"save\")\n",
    "\n",
    "    np.savez(PREPARED_TRAIN_ARRAY, x_train=x_train, y_train=y_train)\n",
    "    np.savez(PREPARED_TEST_ARRAY, x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type and Shape of the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Preparation Completed!\")\n",
    "print(\"Data: \", type(data))\n",
    "print(\"Data: \", type(df))\n",
    "print(\"Index: \", type(df.index))\n",
    "print(\"Train Data: \", train_data.shape)\n",
    "print(\"Test Data: \", test_data.shape)\n",
    "print(\"Train Feature Scaler: \", type(train_feature_scale))\n",
    "print(\"Train Target Scaler: \", type(train_target_scale))\n",
    "print(\"x_train: \", x_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"x_test: \", x_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "data.head(3)\n",
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value counts for the transformed TargetClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df.tail(3)\n",
    "print(type(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the train data and the train data informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the test data and the test data informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected and actual train / test radio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual Ratio: \", split_ratio)\n",
    "print(\"Received train ratio: \", len(train_data) / len(df))\n",
    "print(\"Received test ratio: \", len(test_data) / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The processed close price of the TSLA stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df.index, df['Close'])\n",
    "plt.title(f'{ticker} Stock Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSLA Closing Price with 20 / 100 / 200 days index of EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df.index, df['Close'], label='Close Price')\n",
    "plt.plot(df.index, df['Close_EMA20'], label='20-day EMA', color=\"red\")\n",
    "plt.plot(df.index, df['Close_EMA100'], label='100-day EMA', color=\"orange\")\n",
    "plt.plot(df.index, df['Close_EMA200'], label='200-day EMA', color=\"green\")\n",
    "plt.title(f'{ticker} Stock Price with Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSI Index of the TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df.index, df['Close_RSI'])\n",
    "plt.title(f'{ticker} RSI')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('RSI')\n",
    "plt.axhline(y=70, color='r', linestyle='--')\n",
    "plt.axhline(y=30, color='g', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Candlestick Chart\n",
    "\n",
    "**In short, the draw_candlestick_chart() will take in the dataframe and create a copy, once the copy is created then it will resample the data by the n trading days if n is not None. It then adds in essential technical analysis strategies as additional values to display on the chart and draw the candlestick chart using mplfinance. The following steps display the procedure of the draw_candlestick_chart():**\n",
    "\n",
    "- Dataframe copy: When the dataframe is passed in as an argument, this function will start copying the dataframe using the copy() function provided by pandas. This step is critically essential, as it will prevent any unwanted changes that could be executed to the real dataframe. \n",
    "- Data resample: The draw_candlestick_chart() will then check if the n parameter is not None, if the n parameter is not None then the function will start aggregating the data to find the maximum value of high price, minimum value of low price, first open price value, last close price value and the total of volumes within each resampling n period. Technical Analysis Strategies: Once the resampling procedure is finished, it will add three moving averages (MA) for the ‘Close’ price to the copied dataframe, which are 25 days, 100 days and 200 days moving average.\n",
    "- Create Sub-Plot: The function also creates an array of make_addplot()instances, which will be used to display the moving averages (25 days, 100 days and 200 days) alongside the candlestick chart. However, before the chart is being appended to the array, it must be pre-processed by drop NaN values and reindexed for assuring safe data.\n",
    "- Display the chart: Once the moving averages sub-plot procedure is finished, the function will utilize plot() function from mplfinance to display the candlestick chart. As shown in the codebase, this function will take in the dataframe that we have processed before, with some required parameters, which could be list as the type of the chart, the title of the chart, the style of the chart, the label name, the total volume of the data and the sub-plot that we have created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_candlestick_chart(df: Annotated[pd.DataFrame, Doc(\"The data to be plotted\")],\n",
    "                           n: Annotated[int | None, Doc(\"Resampling period in trading days for aggregation, default is None\")] = None,\n",
    "                           ):\n",
    "    cp_df = df.copy()\n",
    "    if n is not None:\n",
    "        cp_df = cp_df.resample(f'{n}D').agg({\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    cp_df['MA25'] = cp_df['Close'].rolling(window=25).mean()\n",
    "    cp_df['MA100'] = cp_df['Close'].rolling(window=100).mean()\n",
    "    cp_df['MA200'] = cp_df['Close'].rolling(window=200).mean()\n",
    "\n",
    "\n",
    "    sub_chart: mpf.make_addplot  = []\n",
    "    if cp_df.MA25.dropna().shape[0] > 0:\n",
    "        fixed_MA25 = cp_df.MA25.dropna().reindex(cp_df.index, fill_value=None)\n",
    "        sub_chart.append(mpf.make_addplot(fixed_MA25, color='blue'))\n",
    "    if cp_df.MA100.dropna().shape[0] > 0:\n",
    "        fixed_MA100 = cp_df.MA100.dropna().reindex(cp_df.index, fill_value=None)\n",
    "        sub_chart.append(mpf.make_addplot(fixed_MA100, color='orange'))\n",
    "    if cp_df.MA200.dropna().shape[0] > 0:\n",
    "        fixed_MA200 = cp_df.MA200.dropna().reindex(cp_df.index, fill_value=None)\n",
    "        sub_chart.append(mpf.make_addplot(fixed_MA200, color='green'))\n",
    "\n",
    "    mpf.plot(cp_df, type='candle', style='charles', ylabel='Price', ylabel_lower='Volume', volume=True, addplot=sub_chart, title=f'{ticker} Stock Price with Moving Averages')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_candlestick_chart(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_candlestick_chart(data, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_candlestick_chart(data, n=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Box Chart\n",
    "\n",
    "**The process of resampling data and the process of prepare data for the box plot is partially the same as the draw_candlestick_chart(), however, the key difference that separates the draw_box_chart() function is the way that it creates the sub-chart and the libraries that it uses for creating the chart. The below steps show the overall procedure of how the draw_box_chart() function work:**\n",
    "\n",
    "- Dataframe copy: When the dataframe is passed in as an argument, this function will start copying the dataframe using the copy() function provided by pandas. This step is critically essential, as it will prevent any unwanted changes that could be executed to the real dataframe. \n",
    "- Data resample: The draw_candlestick_chart() will then check if the n parameter is not None, if the n parameter is not None then the function will start aggregating the data to find the maximum value of high price, minimum value of low price, first open price value, last close price value and the total of volumes within each resampling n period.\n",
    "- Data preparation: After the data resample is finished, the function will start preparing data for displaying to the plot. First, the two arrays of chart_data and labels are created. The chart_data contains the close, open, low and high price for each data point and the labels will contain the corresponding date for each data point.\n",
    "- Chart display: After preparing the data, the function will start creating and configuring the chart. First, the function will use the subplots() function provided by matplotlib to create a figure and axes. It will then initialize and create the box chart using the boxplot() function (which is also provided by matplotlib). The prepared chart_data array will be used as the main data to draw the box plot with different parameters are set to customize the appearance of the chart, which could be listed as: vertical alignment, patch artist, label , title, label for x and y axis, x-tick interval, the label of the x-tick and the rotation degree of the chart. The k parameter that is defined in the function will be used for determining the interval for displaying the labels at which days. Finally, the show() function of matplotlib is called to display the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box_chart(df: Annotated[pd.DataFrame, Doc(\"The data to be plotted\")],\n",
    "                    n: Annotated[int | None, Doc(\"Resampling period in trading days for aggregation, default is None\")] = None,\n",
    "                    k: Annotated[int, Doc(\"The interval of the box plot, default is 10\")] = 10\n",
    "                    ):\n",
    "    cp_df = df.copy()\n",
    "\n",
    "    if n is not None:\n",
    "        cp_df = cp_df.resample(f'{n}D').agg({\n",
    "            'Open': 'first',\n",
    "            'High': 'max',\n",
    "            'Low': 'min',\n",
    "            'Close': 'last',\n",
    "            'Volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    chart_data: list = []\n",
    "    labels: list = []\n",
    "\n",
    "    for index, row in cp_df.iterrows():\n",
    "        chart_data.append([row['Close'], row['Open'], row['Low'], row['High']])\n",
    "        labels.append(index.strftime('%Y-%m-%d'))\n",
    "\n",
    "    figure, axes = plt.subplots()\n",
    "    axes.boxplot(chart_data, vert=True, patch_artist=True)\n",
    "    axes.set_xticklabels(labels)\n",
    "    axes.set_title(f'{ticker} Box Plot Chart')\n",
    "    axes.set_xlabel('Date')\n",
    "    axes.set_ylabel('Price')\n",
    "    axes.set_xticks(range(1, len(labels) + 1, k))\n",
    "    axes.set_xticklabels(labels[::k], rotation=90)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_box_chart(data, n=None , k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_box_chart(data, n=50, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_box_chart(data, n=15, k=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos30018-env-w1-p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
